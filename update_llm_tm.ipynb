{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnqlmUzrTX23",
        "outputId": "f9bebe70-05d3-4d16-f05f-718ca2fc141c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.1-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.0.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.9/283.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured\n",
            "  Downloading unstructured-0.12.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfminer\n",
            "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.13 (from langchain)\n",
            "  Downloading langchain_community-0.0.13-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.9 (from langchain)\n",
            "  Downloading langchain_core-0.1.13-py3-none-any.whl (228 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.7/228.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Downloading langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.10.0-py2.py3-none-any.whl (457 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.9/457.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.1.2-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.5.0)\n",
            "Collecting unstructured-client>=0.15.1 (from unstructured)\n",
            "  Downloading unstructured_client-0.15.2-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Collecting pycryptodome (from pdfminer)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client>=0.15.1->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.8.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.16.0)\n",
            "Collecting typing-extensions (from unstructured)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
            "Building wheels for collected packages: docx2txt, pdfminer, langdetect\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=9c63a61d7e989d0ed6305cb11424cb9e599f1df95a329407e6539dc1dbecf5f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140056 sha256=c1e38dc6f38893646bd0c9143fd2de655595e1a8b66d73658a518af2f13bf569\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=eb60bbf85a77fb5ae98820a93540dd941db044d74fd44a87541d4c8c1fccbd83\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built docx2txt pdfminer langdetect\n",
            "Installing collected packages: filetype, docx2txt, typing-extensions, rapidfuzz, python-magic, python-iso639, pypdf, pycryptodome, pdf2image, mypy-extensions, marshmallow, langdetect, jsonpointer, jsonpath-python, emoji, backoff, typing-inspect, pdfminer, jsonpatch, langsmith, dataclasses-json, unstructured-client, langchain-core, unstructured, langchain-community, langchain\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 dataclasses-json-0.6.3 docx2txt-0.8 emoji-2.10.0 filetype-1.2.0 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-2.4 langchain-0.1.1 langchain-community-0.0.13 langchain-core-0.1.13 langdetect-1.0.9 langsmith-0.0.83 marshmallow-3.20.2 mypy-extensions-1.0.0 pdf2image-1.17.0 pdfminer-20191125 pycryptodome-3.20.0 pypdf-4.0.0 python-iso639-2024.1.2 python-magic-0.4.27 rapidfuzz-3.6.1 typing-extensions-4.9.0 typing-inspect-0.9.0 unstructured-0.12.2 unstructured-client-0.15.2\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
            "Collecting openai\n",
            "  Downloading openai-1.8.0-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain pypdf unstructured pdf2image docx2txt pdfminer\n",
        "!pip install tiktoken\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d02O8aVrZfdx"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import nltk\n",
        "import pickle\n",
        "from gensim import corpora\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "import re\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6N4ANpCrxrU"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8H4Y1V_sfky",
        "outputId": "1d2193c9-c2cd-4b9b-d521-40e6cedd6f39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'review'], dtype='object')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pos_file = 'data/reviews_llm/negative_reviews_llm.csv'\n",
        "neg_file = 'data/reviews_llm/positive_reviews_llm.csv'\n",
        "df = pd.read_csv(pos_file)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Fq56Yiydzuip",
        "outputId": "36196a9c-9b2d-4aa6-b9be-5597568e3ac6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-15a394ff-69a2-49ac-ac29-d6bd314c6e81\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is a good idea to get off from our station.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I want to improve.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>seems to have increased the number of users i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I think the sound of the name Daimon is also ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>I was surprised.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1381</th>\n",
              "      <td>1381</td>\n",
              "      <td>It is good because the antenna shop changes!C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1382</th>\n",
              "      <td>1382</td>\n",
              "      <td>It is now a soup stock Tokyo, but the curry w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1383</th>\n",
              "      <td>1383</td>\n",
              "      <td>picture is a cafe on the third floor.The staf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1384</th>\n",
              "      <td>1384</td>\n",
              "      <td>Station premises are relatively clean. The go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1385</th>\n",
              "      <td>1385</td>\n",
              "      <td>I like Caldi and Sweet shops.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1386 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15a394ff-69a2-49ac-ac29-d6bd314c6e81')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-15a394ff-69a2-49ac-ac29-d6bd314c6e81 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-15a394ff-69a2-49ac-ac29-d6bd314c6e81');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dadd36f0-7a0e-42cb-aee9-389bbbe2dc10\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dadd36f0-7a0e-42cb-aee9-389bbbe2dc10')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dadd36f0-7a0e-42cb-aee9-389bbbe2dc10 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      Unnamed: 0                                             review\n",
              "0              0       is a good idea to get off from our station. \n",
              "1              1                               I want to improve.1 \n",
              "2              2   seems to have increased the number of users i...\n",
              "3              3   I think the sound of the name Daimon is also ...\n",
              "4              4                                  I was surprised. \n",
              "...          ...                                                ...\n",
              "1381        1381   It is good because the antenna shop changes!C...\n",
              "1382        1382   It is now a soup stock Tokyo, but the curry w...\n",
              "1383        1383   picture is a cafe on the third floor.The staf...\n",
              "1384        1384   Station premises are relatively clean. The go...\n",
              "1385        1385                     I like Caldi and Sweet shops. \n",
              "\n",
              "[1386 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_csv(pos_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP7vJRmMlQEz"
      },
      "outputs": [],
      "source": [
        "def preprocessing(a):\n",
        "  # get rid of non english character\n",
        "  only_en_pos_review = re.sub('[^a-zA-z]', ' ', a)\n",
        "  # change it to lower case\n",
        "  no_cap_pos_review = only_en_pos_review.lower()\n",
        "  # tokenizing\n",
        "  word_tokens = nltk.word_tokenize(no_cap_pos_review)\n",
        "  # pos tagging\n",
        "  tokens_pos = nltk.pos_tag(word_tokens)\n",
        "  # only get NN, VB, JJ (noun, verb, adjective)\n",
        "  final_words = []\n",
        "  for word, pos in tokens_pos:\n",
        "    if  pos == 'NN':\n",
        "      final_words.append(word)\n",
        "    elif pos == 'VB':\n",
        "      final_words.append(word)\n",
        "    elif pos == 'JJ':\n",
        "      final_words.append(word)\n",
        "  # lemmatize\n",
        "  lemma = nltk.WordNetLemmatizer()\n",
        "  lemmatized_words = []\n",
        "  for word in final_words:\n",
        "    new_word = lemma.lemmatize(word)\n",
        "    lemmatized_words.append(new_word)\n",
        "  # stop words\n",
        "  stopwords_list = stopwords.words(['english', 'spanish'])\n",
        "  # adding stop words\n",
        "  add_stopwords_list = ['station', 'seoul', 'yokohama','train', 'line', 'place', 'tokyo', 'newyork',\n",
        "                        'york', 'time', 'kyoto', 'hankyu', 'osaka', 'good', 'shibuya',\n",
        "                        'lot', 'many']\n",
        "  stopwords_list = stopwords_list +  add_stopwords_list\n",
        "\n",
        "  unique_words = set(lemmatized_words)\n",
        "  final_words = lemmatized_words\n",
        "\n",
        "  # get rid of stop words\n",
        "  for word in unique_words:\n",
        "      if word in stopwords_list or len(word) < 3: # delete when it is stopwords or words with two letters\n",
        "          while word in final_words: final_words.remove(word)\n",
        "\n",
        "  return final_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGOHOUr9TdzK"
      },
      "outputs": [],
      "source": [
        "def get_topic_lists_from_csv(file, num_topics, words_per_topic):\n",
        "    \"\"\"\n",
        "    Extracts topics and their associated words from a PDF document using the\n",
        "    Latent Dirichlet Allocation (LDA) algorithm.\n",
        "\n",
        "    Parameters:\n",
        "        file (str): The path to the PDF file for topic extraction.\n",
        "        num_topics (int): The number of topics to discover.\n",
        "        words_per_topic (int): The number of words to include per topic.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of num_topics sublists, each containing relevant words\n",
        "        for a topic.\n",
        "    \"\"\"\n",
        "    # Load the pdf file\n",
        "    '''\n",
        "    loader = CSVLoader(file_path= file, csv_args={\n",
        "      'delimiter': ',',\n",
        "      'quotechar': '\"',\n",
        "      'fieldnames': ['Unnamed: 0', 'statnNm', 'rating', 'review', 'review_count']\n",
        "    })\n",
        "    '''\n",
        "\n",
        "    # Extract the text from each page into a list. Each page is considered a document\n",
        "    data = pd.read_csv(file)\n",
        "    documents= []\n",
        "    for i in data['review']:\n",
        "      documents.append(i)\n",
        "\n",
        "    # Preprocess the documents\n",
        "    processed_documents = [preprocessing(sentence) for sentence in documents]\n",
        "    #processed_documents = [preprocess(doc, stop_words) for doc in documents]\n",
        "    # Create a dictionary and a corpus\n",
        "    dictionary = Dictionary(processed_documents)\n",
        "    dictionary.filter_extremes(keep_n = 2000, no_below=5, no_above=0.5)\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(\n",
        "        corpus,\n",
        "        num_topics=num_topics,\n",
        "        id2word=dictionary,\n",
        "        passes=15,\n",
        "        random_state = 7\n",
        "        )\n",
        "    \n",
        "    # saving model and change the name for the model\n",
        "    # lda_model.save(\"lda_model\")\n",
        "\n",
        "    # Retrieve the topics and their corresponding words\n",
        "    topics = lda_model.print_topics(num_words=words_per_topic)\n",
        "\n",
        "    # Store each list of words from each topic into a list\n",
        "    topics_ls = []\n",
        "    for topic in topics:\n",
        "        print(topic)\n",
        "        words = topic[1].split(\"+\")\n",
        "        topic_words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
        "        topics_ls.append(topic_words)\n",
        "\n",
        "    return topics_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kODBDypxtPxQ"
      },
      "outputs": [],
      "source": [
        "pos_template = '''Describe the postive topic of each of the {num_topics}\n",
        "        double-quote delimited lists in a simple phrase and also write down\n",
        "        ten possible words that can describe the topic. The topics must not\n",
        "        include city name or any pronouns. The lists are the result of an\n",
        "        algorithm for topic discovery.\n",
        "\n",
        "        Do not provide an introduction or a conclusion, only describe the\n",
        "        topics. Do not mention the word \"topic\" when describing the topics.\n",
        "        Use the following template for the response.\n",
        "\n",
        "        1: <<<(sentence describing the topic)>>>\n",
        "        - <<<(whether topic is positive or negative)>>>\n",
        "        - <<<(words describing the topic)>>>\n",
        "\n",
        "        2: <<<(sentence describing the topic)>>>\n",
        "        - <<<(whether topic is positive or negative)>>>\n",
        "        - <<<(words describing the topic)>>>\n",
        "\n",
        "\n",
        "        ...\n",
        "\n",
        "        n: <<<(sentence describing the topic)>>>\n",
        "        - <<<(whether topic is positive or negative)>>>\n",
        "        - <<<(words describing the topic)>>>\n",
        "\n",
        "\n",
        "        Lists: \"\"\"{string_lda}\"\"\" '''\n",
        "\n",
        "neg_template = '''Describe the negative topics of each of the {num_topics}\n",
        "        double-quote delimited lists in a simple phrase and also write down\n",
        "        ten possible words that can describe the topic.The topics must not\n",
        "        include city name or any pronouns. The lists are the result of an\n",
        "        algorithm for topic discovery.\n",
        "\n",
        "        Do not provide an introduction or a conclusion, only describe the\n",
        "        topics. Do not mention the word \"topic\" when describing the topics.\n",
        "        Use the following template for the response.\n",
        "\n",
        "        1: <<<(sentence describing the topic)>>>\n",
        "        - <<<(whether topic is positive or negative)>>>\n",
        "        - <<<(words describing the topic)>>>\n",
        "\n",
        "        2: <<<(sentence describing the topic)>>>\n",
        "        - <<<(whether topic is positive or negative)>>>\n",
        "        - <<<(words describing the topic)>>>\n",
        "\n",
        "\n",
        "        ...\n",
        "\n",
        "        n: <<<(sentence describing the topic)>>>\n",
        "        - <<<(whether topic is positive or negative)>>>\n",
        "        - <<<(words describing the topic)>>>\n",
        "\n",
        "\n",
        "        Lists: \"\"\"{string_lda}\"\"\" '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzeHaZY-9LYj"
      },
      "outputs": [],
      "source": [
        "# 파라미터도 똑같이 맞추었는데 lda_tm_실험실 파일에서 나오는 토픽 단어 분포랑 이 파일에서 나오는 거랑 다르게 나옴.\n",
        "# 일시적인 해결책 => lda_tm_실험실 파일에서 topics_ls 부분을 복사해서 아래 함수 list_of_topicwords에 붙여넣기 통해서 진행\n",
        "# 조금 번거로워서 코드 어떻게 해서 맞춰보겠음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igXlT12VTgy6"
      },
      "outputs": [],
      "source": [
        "def topics_from_csv(llm, template, file, num_topics, words_per_topic):\n",
        "    \"\"\"\n",
        "    Generates descriptive prompts for LLM based on topic words extracted from a\n",
        "    PDF document.\n",
        "\n",
        "    This function takes the output of `get_topic_lists_from_pdf` function,\n",
        "    which consists of a list of topic-related words for each topic, and\n",
        "    generates an output string in table of content format.\n",
        "\n",
        "    Parameters:\n",
        "        llm (LLM): An instance of the Large Language Model (LLM) for generating\n",
        "        responses.\n",
        "        file (str): The path to the PDF file for extracting topic-related words.\n",
        "        num_topics (int): The number of topics to consider.\n",
        "        words_per_topic (int): The number of words per topic to include.\n",
        "\n",
        "    Returns:\n",
        "        str: A response generated by the language model based on the provided\n",
        "        topic words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract topics and convert to string\n",
        "    list_of_topicwords = get_topic_lists_from_csv(file,num_topics,words_per_topic)\n",
        "    string_lda = \"\"\n",
        "    for list in list_of_topicwords:\n",
        "        string_lda += str(list) + \"\\n\"\n",
        "\n",
        "    # Create the template\n",
        "    template_string = template\n",
        "\n",
        "    # LLM call\n",
        "    \n",
        "    prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "    response = chain.run({\n",
        "        \"string_lda\" : string_lda,\n",
        "        \"num_topics\" : num_topics\n",
        "        })\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0nzCta1TmGN",
        "outputId": "344bba1f-497b-4b45-a768-20c09e3b1430"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "openai_key = '' # use personal key\n",
        "llm = OpenAI(openai_api_key=openai_key, max_tokens=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcH271bETosB",
        "outputId": "47be82f2-744b-4276-d51b-2ca51fbe1e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['station', 'good', 'line', 'airport', 'convenient', 'subway', 'many', 'train', 'lot', 'place']\n",
            "['station', 'good', 'new', 'train', 'place', 'subway', 'clean', 'beautiful', 'many', 'city']\n",
            "['station', 'good', 'many', 'lot', 'delicious', 'enjoy', 'day', 'high', 'shop', 'walk']\n",
            "['station', 'line', 'good', 'convenient', 'clean', 'many', 'place', 'easy', 'transfer', 'use']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# change number of topics\n",
        "\n",
        "num_topics = 4\n",
        "# num_topics = 5\n",
        "# num_topics = 6\n",
        "words_per_topic = 30\n",
        "\n",
        "summary = topics_from_csv(llm, pos_template, pos_file, num_topics, words_per_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "vT2MDKCoTsFl",
        "outputId": "01fbec46-49f9-4ae7-943f-9efdf2ffbe14"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1: <<<(A convenient and well-connected transportation hub)>>>\\n- <<<(positive)>>>\\n- <<<(station, good, line, airport, convenient, subway, many, train, lot, place)>>>\\n\\n2: <<<(A modern and efficient train station)>>>\\n- <<<(positive)>>>\\n- <<<(station, good, new, train, place, subway, clean, beautiful, many, city)>>>\\n\\n3: <<<(A bustling and lively food market)>>>\\n- <<<(positive)>>>\\n- <<<(station, good, many, lot, delicious, enjoy, day, high, shop, walk)>>>\\n\\n4: <<<(An easy and efficient subway system)>>>\\n- <<<(positive)>>>\\n- <<<(station, line, good, convenient, clean, many, place, easy, transfer, use)>>>\\n'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34iGA4WJVX4i",
        "outputId": "eea5d2e5-a493-4816-add7-0871580610a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['station', 'ticket', 'place', 'good', 'difficult', 'gate', 'elevator', 'lot', 'home', 'old']\n",
            "['station', 'train', 'lot', 'subway', 'good', 'many', 'dirty', 'street', 'new', 'night']\n",
            "['station', 'line', 'transfer', 'many', 'train', 'bad', 'good', 'subway', 'use', 'exit']\n",
            "['bad', 'high', 'infection', 'narrow', 'station', 'shop', 'store', 'risk', 'business', 'area']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# num_topics = 4\n",
        "# num_topics = 5\n",
        "num_topics = 6\n",
        " \n",
        "words_per_topic = 30\n",
        "\n",
        "summary = topics_from_csv(llm, neg_template, neg_file, num_topics, words_per_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "2CP5GfbKs9vJ",
        "outputId": "ea080232-d09c-41a6-b751-dab499bba3a7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n1: <<<The station is a difficult and crowded place that is often confusing and overwhelming.>>>\\n- Negative\\n- Difficult, crowded, confusing, overwhelming, busy, hectic, chaotic, stressful, frustrating, overwhelming\\n\\n2: <<<Taking the train can be a dirty and unpleasant experience, especially at night.>>>\\n- Negative\\n- Dirty, unpleasant, crowded, busy, chaotic, stressful, frustrating, overwhelming, uncomfortable, unclean\\n\\n3: <<<Navigating the station can be a challenge with its many lines and transfers, making it difficult to use.>>>\\n- Negative\\n- Challenging, confusing, difficult, overwhelming, crowded, chaotic, stressful, frustrating, complicated, overwhelming\\n\\n4: <<<The station area has a high risk of infection due to its narrow and crowded streets, making it a bad place to do business.>>>\\n- Negative\\n- High risk, infection, narrow, crowded, dirty, unclean, unhealthy, unsanitary, hazardous, polluted'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
